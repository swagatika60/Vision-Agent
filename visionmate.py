import streamlit as st
import cv2
import numpy as np
from PIL import Image
import json
import os
import bcrypt
import base64
from io import BytesIO
import tempfile
import time
from datetime import datetime
import asyncio
import nest_asyncio
from dotenv import load_dotenv
import gc

nest_asyncio.apply()
from ui_components import VisionMateUI
load_dotenv()

from vision_agents.core import Agent, User
from vision_agents.plugins import getstream, gemini, ultralytics as agent_ultralytics

try:
Â  Â  from ultralytics import YOLO
Â  Â  import torch
Â  Â  from gtts import gTTS
except ImportError as e:
Â  Â  st.error(f"Missing required dependency: {e}. Please run pip install -r requirements.txt")
Â  Â  st.stop()

# ==========================================
# STATE INITIALIZATION
# ==========================================

def init_session_state():
Â  Â  defaults = {
Â  Â  Â  Â  'authenticated': False,
Â  Â  Â  Â  'username': None,
Â  Â  Â  Â  'user_name': None,
Â  Â  Â  Â  'audio_enabled': True,
Â  Â  Â  Â  'models_loaded': False,
Â  Â  Â  Â  'current_page': 'home',
Â  Â  Â  Â  'welcome_played': False
Â  Â  }
Â  Â  for key, value in defaults.items():
Â  Â  Â  Â  if key not in st.session_state:
Â  Â  Â  Â  Â  Â  st.session_state[key] = value

init_session_state()
USER_DATA_FILE = "users.json"

# ==========================================
# AUTHENTICATION LAYER
# ==========================================

def get_db():
Â  Â  if os.path.exists(USER_DATA_FILE):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  with open(USER_DATA_FILE, 'r') as f:
Â  Â  Â  Â  Â  Â  Â  Â  return json.load(f)
Â  Â  Â  Â  except json.JSONDecodeError:
Â  Â  Â  Â  Â  Â  return {}
Â  Â  return {}

def save_db(data):
Â  Â  with open(USER_DATA_FILE, 'w') as f:
Â  Â  Â  Â  json.dump(data, f, indent=4)

def hash_pw(password):
Â  Â  return bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')

def verify_pw(password, hashed):
Â  Â  return bcrypt.checkpw(password.encode('utf-8'), hashed.encode('utf-8'))

def handle_registration(username, name, password):
Â  Â  db = get_db()
Â  Â  if username in db:
Â  Â  Â  Â  return False, "Username already exists."
Â  Â  db[username] = {
Â  Â  Â  Â  'password': hash_pw(password),
Â  Â  Â  Â  'name': name,
Â  Â  Â  Â  'created_at': datetime.now().isoformat()
Â  Â  }
Â  Â  save_db(db)
Â  Â  return True, "Registration successful."

def handle_login(username, password):
Â  Â  db = get_db()
Â  Â  if username not in db:
Â  Â  Â  Â  return False, "Invalid credentials.", None
Â  Â Â 
Â  Â  user_data = db[username]
Â  Â  hashed_password = user_data.get('password') if isinstance(user_data, dict) else user_data
Â  Â  display_name = user_data.get('name', username) if isinstance(user_data, dict) else username
Â  Â Â 
Â  Â  if verify_pw(password, hashed_password):
Â  Â  Â  Â  return True, "Login successful.", display_name
Â  Â  return False, "Invalid credentials.", None

# ==========================================
# UTILITY FUNCTIONS
# ==========================================

def generate_audio_feedback(text):
Â  Â  """Generates base64 encoded audio player for UI feedback."""
Â  Â  try:
Â  Â  Â  Â  tts = gTTS(text=text, lang='en', slow=False)
Â  Â  Â  Â  fp = BytesIO()
Â  Â  Â  Â  tts.write_to_fp(fp)
Â  Â  Â  Â  fp.seek(0)
Â  Â  Â  Â  b64 = base64.b64encode(fp.read()).decode()
Â  Â  Â  Â  return f'<audio autoplay><source src="data:audio/mp3;base64,{b64}" type="audio/mp3"></audio>'
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Audio generation failed: {e}")
Â  Â  Â  Â  return None

# ==========================================
# CORE VISION ENGINE
# ==========================================

@st.cache_resource
def initialize_vision_model():
Â  Â  """Caches YOLOv8s model to prevent memory leaks during standard ops."""
Â  Â  try:
Â  Â  Â  Â  return YOLO('yolov8s.pt')
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Model initialization failed: {e}")
Â  Â  Â  Â  return None

def resize_maintaining_aspect(image, target_width=640):
Â  Â  """Dynamically scales frame to target width while preserving aspect ratio for detection accuracy."""
Â  Â  h, w = image.shape[:2]
Â  Â  if w > target_width:
Â  Â  Â  Â  ratio = target_width / w
Â  Â  Â  Â  return cv2.resize(image, (target_width, int(h * ratio)))
Â  Â  return image

def process_bounding_boxes(image, results):
Â  Â  """Draws boxes and calculates dynamic proximity hazards based on vertical span."""
Â  Â  annotated = image.copy()
Â  Â  img_h = image.shape[0]
Â  Â Â 
Â  Â  if results.boxes.id is None and len(results.boxes) == 0:
Â  Â  Â  Â  return annotated

Â  Â  for box in results.boxes:
Â  Â  Â  Â  x1, y1, x2, y2 = map(int, box.xyxy[0])
Â  Â  Â  Â  cls_idx = int(box.cls[0])
Â  Â  Â  Â  name = results.names[cls_idx]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Spatial calculation: object height relative to frame height
Â  Â  Â  Â  vertical_span = (y2 - y1) / img_h
Â  Â  Â  Â Â 
Â  Â  Â  Â  if vertical_span > 0.4:
Â  Â  Â  Â  Â  Â  status, color, thickness = "HAZARD", (0, 0, 255), 4
Â  Â  Â  Â  elif vertical_span > 0.2:
Â  Â  Â  Â  Â  Â  status, color, thickness = "MID", (0, 255, 255), 2
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  status, color, thickness = "FAR", (0, 255, 0), 2
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  label = f"{name} [{status}]"
Â  Â  Â  Â  cv2.rectangle(annotated, (x1, y1), (x2, y2), color, thickness)
Â  Â  Â  Â  cv2.putText(annotated, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, max(1, thickness-1))
Â  Â  Â  Â Â 
Â  Â  return annotated

# ==========================================
# UI ROUTING & PAGES
# ==========================================

def render_auth():
Â  Â  VisionMateUI.load_css()
Â  Â  VisionMateUI.welcome_banner()
Â  Â  tab1, tab2 = VisionMateUI.auth_tabs()
Â  Â Â 
Â  Â  with tab1:
Â  Â  Â  Â  st.subheader("Account Access")
Â  Â  Â  Â  with st.form("login_form"):
Â  Â  Â  Â  Â  Â  u = st.text_input("Username")
Â  Â  Â  Â  Â  Â  p = st.text_input("Password", type="password")
Â  Â  Â  Â  Â  Â  if st.form_submit_button("Login", type="primary"):
Â  Â  Â  Â  Â  Â  Â  Â  success, msg, name = handle_login(u, p)
Â  Â  Â  Â  Â  Â  Â  Â  if success:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.authenticated = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.username = u
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.user_name = name
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  else: st.error(msg)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  with tab2:
Â  Â  Â  Â  st.subheader("New Registration")
Â  Â  Â  Â  with st.form("reg_form"):
Â  Â  Â  Â  Â  Â  nu = st.text_input("Username")
Â  Â  Â  Â  Â  Â  nn = st.text_input("Display Name")
Â  Â  Â  Â  Â  Â  np_pw = st.text_input("Password", type="password")
Â  Â  Â  Â  Â  Â  if st.form_submit_button("Register"):
Â  Â  Â  Â  Â  Â  Â  Â  success, msg = handle_registration(nu, nn, np_pw)
Â  Â  Â  Â  Â  Â  Â  Â  if success: st.success(msg)
Â  Â  Â  Â  Â  Â  Â  Â  else: st.error(msg)

def render_dashboard():
Â  Â  VisionMateUI.load_css()
Â  Â  VisionMateUI.welcome_banner(st.session_state.user_name)
Â  Â Â 
Â  Â  if not st.session_state.welcome_played and st.session_state.audio_enabled:
Â  Â  Â  Â  st.markdown(generate_audio_feedback(f"Welcome back, {st.session_state.user_name}"), unsafe_allow_html=True)
Â  Â  Â  Â  st.session_state.welcome_played = True
Â  Â Â 
Â  Â  col1, col2, col3 = st.columns(3)
Â  Â  with col1:
Â  Â  Â  Â  st.markdown(VisionMateUI.mode_card("Image Processing", "Static analysis", "ðŸ“·"), unsafe_allow_html=True)
Â  Â  Â  Â  if st.button("Launch Image Engine", use_container_width=True):
Â  Â  Â  Â  Â  Â  st.session_state.current_page = 'image'; st.rerun()
Â  Â  with col2:
Â  Â  Â  Â  st.markdown(VisionMateUI.mode_card("Live Protocol", "Agent WebRTC", "ðŸŽ¥"), unsafe_allow_html=True)
Â  Â  Â  Â  if st.button("Launch Agent Room", use_container_width=True):
Â  Â  Â  Â  Â  Â  st.session_state.current_page = 'live'; st.rerun()
Â  Â  with col3:
Â  Â  Â  Â  st.markdown(VisionMateUI.mode_card("Video Pipeline", "Live telemetry scan", "ðŸŽ¬"), unsafe_allow_html=True)
Â  Â  Â  Â  if st.button("Launch Video Pipeline", use_container_width=True):
Â  Â  Â  Â  Â  Â  st.session_state.current_page = 'video'; st.rerun()

def view_image_analysis():
Â  Â  VisionMateUI.load_css()
Â  Â  VisionMateUI.page_header("Image Processing", "High-Fidelity Frame Inspection", "ðŸ“·")
Â  Â  if st.button("â† Return to Dashboard"):Â 
Â  Â  Â  Â  st.session_state.current_page = 'home'; st.rerun()
Â  Â Â 
Â  Â  upload = st.file_uploader("Select media", type=['jpg', 'png', 'jpeg'])
Â  Â  if upload:
Â  Â  Â  Â  img = Image.open(upload).convert("RGB")
Â  Â  Â  Â  _, col_main, _ = st.columns([1, 2, 1])
Â  Â  Â  Â  with col_main:
Â  Â  Â  Â  Â  Â  st.image(img, use_container_width=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if st.button("Execute Deep Scan", type="primary", use_container_width=True):
Â  Â  Â  Â  Â  Â  img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
Â  Â  Â  Â  Â  Â  img_cv = resize_maintaining_aspect(img_cv, 800)
Â  Â  Â  Â  Â  Â  h = img_cv.shape[0]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with st.spinner("Processing structural tensors..."):
Â  Â  Â  Â  Â  Â  Â  Â  results = st.session_state.yolo_model(img_cv, conf=0.25, iou=0.45)[0]
Â  Â  Â  Â  Â  Â  Â  Â  annotated = process_bounding_boxes(img_cv, results)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  hazards = []
Â  Â  Â  Â  Â  Â  Â  Â  freq_map = {}
Â  Â  Â  Â  Â  Â  Â  Â  for box in results.boxes:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  obj_class = results.names[int(box.cls[0])]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  freq_map[obj_class] = freq_map.get(obj_class, 0) + 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (int(box.xyxy[0][3]) - int(box.xyxy[0][1])) / h > 0.4:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hazards.append(obj_class)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if hazards:
Â  Â  Â  Â  Â  Â  Â  Â  alert = f"HAZARD DETECTED: {', '.join(set(hazards))} proximity threshold breached."
Â  Â  Â  Â  Â  Â  Â  Â  st.error(alert)
Â  Â  Â  Â  Â  Â  Â  Â  if st.session_state.audio_enabled: st.markdown(generate_audio_feedback(alert), unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  VisionMateUI.image_comparison(img, cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB), "Source", "Processed")
Â  Â  Â  Â  Â  Â  str_counts = ", ".join([f"{v} {k}" for k, v in freq_map.items()])
Â  Â  Â  Â  Â  Â  desc = f"Scan complete. Classified elements: {str_counts}." if freq_map else "No structured elements classified."
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown(VisionMateUI.info_card("Telemetry", desc, "success"), unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  if st.session_state.audio_enabled and not hazards:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(generate_audio_feedback(desc), unsafe_allow_html=True)

async def init_agent_webrtc(user_name):
Â  Â  """Establishes stream edge connection for native low-latency agent."""
Â  Â  processor = agent_ultralytics.YOLOPoseProcessor(model_path="yolov8s-pose.pt")
Â  Â  agent = Agent(
Â  Â  Â  Â  edge=getstream.Edge(),
Â  Â  Â  Â  agent_user=User(name="VisionMate", id="sys_agent"),
Â  Â  Â  Â  instructions=f"Provide spatial awareness and mobility assistance for {user_name}.",
Â  Â  Â  Â  llm=gemini.Realtime(),Â 
Â  Â  Â  Â  processors=[processor]Â 
Â  Â  )
Â  Â  await agent.create_user()
Â  Â  call = await agent.create_call("default", "visionmate-demo-room")
Â  Â  async with agent.join(call):
Â  Â  Â  Â  await agent.finish()

def view_live_protocol():
Â  Â  VisionMateUI.load_css()
Â  Â  VisionMateUI.page_header("Live Protocol", "Real-Time Agent Interface", "ðŸŽ¥")
Â  Â  if st.button("â† Return to Dashboard"):Â 
Â  Â  Â  Â  st.session_state.current_page = 'home'; st.rerun()
Â  Â Â 
Â  Â  st.markdown("---")
Â  Â  st.subheader("Native Hardware Test")
Â  Â  st.info("Execute proximity calculations via standard browser camera access.")
Â  Â Â 
Â  Â  cam_buffer = st.camera_input("Initialize hardware stream")
Â  Â  if cam_buffer:
Â  Â  Â  Â  with st.spinner("Processing telemetry..."):
Â  Â  Â  Â  Â  Â  img = Image.open(cam_buffer).convert("RGB")
Â  Â  Â  Â  Â  Â  cv_img = resize_maintaining_aspect(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR), 640)
Â  Â  Â  Â  Â  Â  h = cv_img.shape[0]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  res = st.session_state.yolo_model(cv_img, conf=0.3)[0]
Â  Â  Â  Â  Â  Â  ann = process_bounding_boxes(cv_img, res)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  hazards = [res.names[int(b.cls[0])] for b in res.boxes if (int(b.xyxy[0][3]) - int(b.xyxy[0][1])) / h > 0.4]
Â  Â  Â  Â  Â  Â  freq = {}
Â  Â  Â  Â  Â  Â  for b in res.boxes: freq[res.names[int(b.cls[0])]] = freq.get(res.names[int(b.cls[0])], 0) + 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.image(cv2.cvtColor(ann, cv2.COLOR_BGR2RGB), caption="Processed Buffer", use_container_width=True)
Â  Â  Â  Â  Â  Â  desc = "Detected: " + ", ".join([f"{v} {k}" for k, v in freq.items()]) if freq else "Clear."
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown(VisionMateUI.info_card("Environment Data", desc, "success"), unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  if hazards: st.error(f"CRITICAL: {', '.join(set(hazards))} breaching safe zone.")

Â  Â  st.markdown("---")
Â  Â  st.subheader("WebRTC Edge Network")
Â  Â  st.caption("Bypasses standard HTTP latency. Uses Stream's protocol for <30ms response times.")
Â  Â Â 
Â  Â  webrtc_uri = "https://demo.visionagents.ai/join?call_id=visionmate-demo-room"
Â  Â  st.markdown(f"**[Authenticate WebRTC Client]({webrtc_uri})**")
Â  Â Â 
Â  Â  if st.button("Initialize Agent Socket"):
Â  Â  Â  Â  with st.spinner("Establishing WebSocket connection..."):
Â  Â  Â  Â  Â  Â  try: asyncio.run(init_agent_webrtc(st.session_state.user_name))
Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"Socket binding failed: {e}")

def view_video_pipeline():
Â  Â  VisionMateUI.load_css()
Â  Â  VisionMateUI.page_header("Video Pipeline", "Time-Series Hazard Tracking", "ðŸŽ¬")
Â  Â  if st.button("â† Return to Dashboard"):Â 
Â  Â  Â  Â  st.session_state.current_page = 'home'; st.rerun()
Â  Â Â 
Â  Â  st.markdown("---")
Â  Â  upload = st.file_uploader("Select environment recording", type=['mp4', 'avi', 'mov'])
Â  Â Â 
Â  Â  if upload:
Â  Â  Â  Â  tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
Â  Â  Â  Â  tmp.write(upload.read())
Â  Â  Â  Â  path = tmp.name
Â  Â  Â  Â Â 
Â  Â  Â  Â  _, col_vid, _ = st.columns([1, 2, 1])
Â  Â  Â  Â  with col_vid: st.video(path)
Â  Â  Â  Â Â 
Â  Â  Â  Â  cap = cv2.VideoCapture(path)
Â  Â  Â  Â  frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
Â  Â  Â  Â  fps = int(cap.get(cv2.CAP_PROP_FPS))
Â  Â  Â  Â  cap.release()
Â  Â  Â  Â Â 
Â  Â  Â  Â  c1, c2, c3 = st.columns(3)
Â  Â  Â  Â  with c1: st.metric("Duration", f"{frames / max(fps, 1):.1f}s")
Â  Â  Â  Â  with c2: st.metric("Total Frames", frames)
Â  Â  Â  Â  with c3: st.metric("Framerate", fps)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â Â 
Â  Â  Â  Â  if st.button("Execute Live Pipeline", type="primary", use_container_width=True):
Â  Â  Â  Â  Â  Â  cap = cv2.VideoCapture(path)
Â  Â  Â  Â  Â  Â  pb = st.progress(0)
Â  Â  Â  Â  Â  Â  sys_log = st.empty()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- SYNCHRONOUS LIVE DASHBOARD LAYOUT ---
Â  Â  Â  Â  Â  Â  col_stream, col_telemetry = st.columns([2, 1])
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with col_stream:
Â  Â  Â  Â  Â  Â  Â  Â  stream_disp = st.empty()
Â  Â  Â  Â  Â  Â  Â  Â  hud_disp = st.empty()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with col_telemetry:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ðŸ“¡ Live Telemetry")
Â  Â  Â  Â  Â  Â  Â  Â  metric_disp = st.empty()
Â  Â  Â  Â  Â  Â  Â  Â  timeline_disp = st.empty()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  hazards, safe, cache = set(), set(), set()
Â  Â  Â  Â  Â  Â  timeline = []
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  step_rate = max(1, fps // 5)Â 
Â  Â  Â  Â  Â  Â  idx = 0
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  sys_log.info("Pipeline initialized. Compiling tensors...")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  while cap.isOpened():
Â  Â  Â  Â  Â  Â  Â  Â  ret, frm = cap.read()
Â  Â  Â  Â  Â  Â  Â  Â  if not ret: break
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if idx % step_rate == 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  frm = resize_maintaining_aspect(frm, 640)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  h = frm.shape[0]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  res = st.session_state.yolo_model.track(frm, persist=True, verbose=False, conf=0.25, iou=0.45)[0]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ann = frm.copy()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  frame_cache = {}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if res.boxes.id is not None or len(res.boxes) > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for b in res.boxes:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cls = res.names[int(b.cls[0])]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cache.add(cls)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  frame_cache[cls] = frame_cache.get(cls, 0) + 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  x1, y1, x2, y2 = map(int, b.xyxy[0])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (y2 - y1) / h > 0.4:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hazards.add(cls)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv2.rectangle(ann, (x1, y1), (x2, y2), (0, 0, 255), 4)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv2.putText(ann, f"HAZARD: {cls}", (x1, y1-15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 3)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  safe.add(cls)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv2.rectangle(ann, (x1, y1), (x2, y2), (0, 255, 0), 2)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv2.putText(ann, cls, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Store Timeline Data
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if frame_cache:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  timeline.append({'ts': idx / max(fps, 1), 'data': frame_cache})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hud_str = " | ".join([f"{v} {k}" for k, v in frame_cache.items()])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hud_str = "Null output"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Video HUD Draw
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  h_hud = int(h * 0.08)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv2.rectangle(ann, (0, 0), (ann.shape[1], h_hud), (0, 0, 0), -1)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv2.putText(ann, f"TELEMETRY: {hud_str}", (10, int(h_hud*0.7)), cv2.FONT_HERSHEY_SIMPLEX, h*0.0015, (255, 255, 255), max(1, int(h*0.003)))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- LIVE UI UPDATES ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stream_disp.image(cv2.cvtColor(ann, cv2.COLOR_BGR2RGB), use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hud_disp.markdown(f"**<div align='center'>Network Data: {hud_str}</div>**", unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with metric_disp.container():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  m1, m2 = st.columns(2)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  m1.metric("Unique Types", len(cache))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  m2.metric("Total Hazards", len(hazards))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  log_txt = "#### Time-Series Log\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Display the 6 most recent logs during playback
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for t in reversed(timeline[-6:]):Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ts = f"{int(t['ts']//60):02d}:{int(t['ts']%60):02d}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  obj = ", ".join([f"{v} {k}" for k, v in t['data'].items()])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  log_txt += f"ðŸ”´ **{ts}** - {obj}\n\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  timeline_disp.markdown(log_txt)

Â  Â  Â  Â  Â  Â  Â  Â  idx += 1
Â  Â  Â  Â  Â  Â  Â  Â  pb.progress(min(idx / frames, 1.0))
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  cap.release()
Â  Â  Â  Â  Â  Â  pb.progress(1.0)
Â  Â  Â  Â  Â  Â  sys_log.success("Pipeline execution complete.")
Â  Â  Â  Â  Â  Â  hud_disp.empty()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- FINAL SUMMARY ---
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  st.markdown("### Aggregated Telemetry")
Â  Â  Â  Â  Â  Â  if cache:
Â  Â  Â  Â  Â  Â  Â  Â  st.write("**Classes Detected:** ", ", ".join(list(cache)))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if hazards:
Â  Â  Â  Â  Â  Â  Â  Â  warn = f"CRITICAL: {', '.join(hazards)} recorded within minimum safe distance."
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(VisionMateUI.info_card("Security Audit", warn, "warning"), unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  warn = "Audit passed. Zero hazards recorded."
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(VisionMateUI.info_card("Security Audit", warn, "success"), unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if st.session_state.audio_enabled: st.markdown(generate_audio_feedback(warn), unsafe_allow_html=True)

Â  Â  Â  Â  Â  Â  rpt = {
Â  Â  Â  Â  Â  Â  Â  Â  "status": "CRITICAL" if hazards else "NOMINAL",
Â  Â  Â  Â  Â  Â  Â  Â  "hazards": list(hazards),
Â  Â  Â  Â  Â  Â  Â  Â  "entities": list(cache),
Â  Â  Â  Â  Â  Â  Â  Â  "timestamp": datetime.now().isoformat()
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  st.download_button("Export System Report (.json)", data=json.dumps(rpt, indent=4), file_name="visionmate_audit.json", mime="application/json")
Â  Â  Â  Â  Â  Â  gc.collect()

# ==========================================
# BOOTSTRAP
# ==========================================

def main():
Â  Â  st.set_page_config(page_title="VisionMate Core", layout="wide")
Â  Â Â 
Â  Â  if not st.session_state.authenticated:
Â  Â  Â  Â  render_auth()
Â  Â  else:
Â  Â  Â  Â  with st.sidebar:
Â  Â  Â  Â  Â  Â  VisionMateUI.sidebar_header(st.session_state.user_name)
Â  Â  Â  Â  Â  Â  st.session_state.audio_enabled = st.checkbox("System Audio", value=True)
Â  Â  Â  Â  Â  Â  if st.button("Dashboard"): st.session_state.current_page = 'home'; st.rerun()
Â  Â  Â  Â  Â  Â  if st.button("Terminate Session"):Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.authenticated = False; st.rerun()

Â  Â  Â  Â  if not st.session_state.models_loaded:
Â  Â  Â  Â  Â  Â  with st.spinner("Initializing structural arrays..."):
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.yolo_model = initialize_vision_model()
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.models_loaded = True
Â  Â  Â  Â Â 
Â  Â  Â  Â  pg = st.session_state.current_page
Â  Â  Â  Â  if pg == 'home': render_dashboard()
Â  Â  Â  Â  elif pg == 'image': view_image_analysis()
Â  Â  Â  Â  elif pg == 'live': view_live_protocol()
Â  Â  Â  Â  elif pg == 'video': view_video_pipeline()

if __name__ == "__main__":
Â  Â  main()
